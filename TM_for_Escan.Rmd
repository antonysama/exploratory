---
title: "Scalable Text Mining for Large Escanning Datsets"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

##Synopsis: 

##Harvested the original data of approximately 5500 e-scanned articles from IBM Watson to the E-scanning database (called Mongo), and saved in disk drive. Sandard text processing and cleaning ("the", "and", "ing", ";", ":" etc.,) simplified the ensuing  three types text analyses using R text mining  packages.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=F}
library("tm")
library ("SnowballC")  
```

```{r include=F}
cname <- file.path("C:", "texts")
docs <- Corpus(DirSource(cname))
```


## DATA PROCESSING

```{r include=T}
docs <- tm_map(docs, removePunctuation) 
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace) 
docs <- tm_map(docs, stemDocument) 
dtm <- DocumentTermMatrix(docs) 
tdm <- TermDocumentMatrix(docs) 
mystopwords <- findFreqTerms(tdm, 1, 10)#cut less frquent words
mystpwrds <- paste(mystopwords, collapse = "|")
tdm <- tdm[tdm$dimnames$Terms[!grepl(mystpwrds,tdm$dimnames$Terms)],]
morestpwds<-c("alchemyapi", "said", "will")
morestpwds<-paste(morestpwds, collapse="|")
tdm <- tdm[tdm$dimnames$Terms[!grepl(morestpwds,tdm$dimnames$Terms)],]
tdmss <- removeSparseTerms(tdm, 0.06)  
```




## CLUSTERING

## Creating a hierrarchichal cluster dendrogram  using rafalib

```{r include=T}
d<-dist(tdm)
#rownames(d) <- tdm
hc<-hclust(d)
#plot(hc)
library("rafalib")
myplclust(hc, labels=hc$labels)
rect.hclust(hc,k=3)

```
